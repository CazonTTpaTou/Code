import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

import scipy
from statsmodels.graphics.gofplots import qqplot
from scipy.stats import shapiro
from scipy.stats import anderson
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss

from statsmodels.tsa.arima_model import ARIMA

path = 'c://users//monne//desktop'
fileName = 'Data_Maintenance.csv'

filePath = '//'.join([path,fileName])

data_maintenance = pd.read_csv(filePath,delimiter=';')

print(data_maintenance.head(5))

"""
data_maintenance.TTV.hist(label="Distribution of wafers thickness")
plt.plot([95, 95], [0, 3500],
         color='red',
         linestyle='--',
         linewidth=3,
         label="Out 6-sigma limit")

plt.legend(loc='upper right');
plt.show()

sns.distplot(data_maintenance.TTV)
plt.show()

g = sns.FacetGrid(data_maintenance, 
                  col="SiliciumType", 
                  size=6)

g.map(sns.kdeplot,
      "TTV",
      shade=True)

plt.xlabel('TTV')
plt.show()

qqplot(data_maintenance.TTV, line='s')
plt.show()

stats,p_value = shapiro(data_maintenance.TTV)
print('Stats = {} - p-value = {}'.format(stats,p_value))

# D'Agostino and Pearson's Test
print(scipy.stats.normaltest(data_maintenance.TTV))

# Anderson's Test
result = anderson(data_maintenance.TTV)
print('Statistic: %.3f' % result.statistic)
for i in range(len(result.critical_values)):
    sl, cv = result.significance_level[i], result.critical_values[i]
	
    if result.statistic < result.critical_values[i]:
        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))
    else:
        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))

print(mean_TTV)

data_maintenance['year'] = data_maintenance['Date'].apply(lambda x:x[len(x)-4:])
data_maintenance['month'] = data_maintenance['Date'].apply(lambda x:x[len(x)-7:len(x)-5])
data_maintenance['day'] = data_maintenance['Date'].apply(lambda x:x[:2])
     
data_maintenance['date_py'] = pd.to_datetime(data_maintenance[['year','month','day']])

grouped = data_maintenance.groupby('date_py')
mean_TTV = grouped['TTV'].agg(np.mean)
print(mean_TTV)
mean_TTV.plot()
plt.show()

print('--'*20)
print('Augmented Dickey Fuller Test')

results = adfuller(mean_TTV,
                   autolag='AIC')

test_statistics,p_value = results[0],results[1]

print('valeur du test : {} - Valeur de la p-value : {}'.format(test_statistics,
                                                          p_value))

for key,value in results[4].items():
    print('Critical value à {} % : {}'.format(key,value))
print('--'*20)    
if p_value < 0.05:
    print('Reject H0 : the serie of fluctuations of TTV is stationnary...') 
else:
    print('Fail to reject H0 : the serie of fluctuations of TTV is not stationnary...') 
    
print('--'*20)
print('Kwiatkowski-Phillips-Schmidt-Shin Test')    


results = kpss(mean_TTV,
                   regression='c')

test_statistics,p_value = results[0],results[1]

print('valeur du test : {} - Valeur de la p-value : {}'.format(test_statistics,
                                                          p_value))

for key,value in results[3].items():
    print('Critical value à {} % : {}'.format(key,value))
print('--'*20)    
if p_value > 0.05:
    print('Fail to reject H0 : the serie of fluctuations of TTV is stationnary...') 
else:
    print('Reject H0 : the serie of fluctuations of TTV is not stationnary...') 

# Forecast with ARIMA model       
model = ARIMA(mean_TTV, order=(2,0, 0))  
results_ARIMA = model.fit()  

plt.plot(mean_TTV)
plt.plot(results_ARIMA.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-mean_TTV)**2))

"""

is_emptying =list()
cumul_wafers = list()
day_since_emptying = list()
cumul_ant=0

for index,row in data_maintenance.iterrows():    
    if row['Date'][:2]=='01':
        is_emptying.append(1)
        cumul_wafers.append(0)
        cumul_ant=row['WafersProduction']
        day=int(row['LastEmptying'][:2])
        day_since_emptying.append(0)
        
    else:
        is_emptying.append(0)
        cumul_wafers.append(row['WafersProduction']-cumul_ant)
        day_since_emptying.append(np.abs(int(row['Date'][:2])-day))
        
data_maintenance['day of emptying']  = is_emptying      
data_maintenance['cumul of wafers']  = cumul_wafers  
data_maintenance['day since emptying']  = day_since_emptying  
data_maintenance['Is Mono'] = data_maintenance['SiliciumType'].apply(lambda x: 1 if x=='Mono' else 0)

print(data_maintenance.head(5))
print(data_maintenance.tail(5))

#plt.matshow(data_maintenance.corr())

for col in data_maintenance.corr().columns:
    conditions = (data_maintenance.corr()[col]>0.3) & (data_maintenance.corr()[col]<1)
    columns_correlated = data_maintenance.corr()[conditions][col]
    print('--'*20)
    print(col)
    print('--'*20)
    print(columns_correlated)

from sklearn.preprocessing import StandardScaler

features = data_maintenance.corr().columns.tolist()
features.remove('OutSixSigma')
target = 'OutSixSigma'

X = data_maintenance.loc[:,features]
y = data_maintenance.loc[:,target]

X = StandardScaler().fit_transform(X)

from sklearn.decomposition import PCA

pca = PCA(n_components=4)
principal_components = pca.fit_transform(X)

principalDf = pd.DataFrame(data = principal_components,
                           columns = ['PCI' + str(i) for i in range(4)])

plt.scatter(principalDf.iloc[:,0],
            principalDf.iloc[:,1],
            c=y,
            label=y.unique())

print('Variances explained by axes : {}'.format(pca.explained_variance_ratio_))

plt.show()

sortedFeatures_axe1 = [i[0] for i in sorted(zip(features,pca.components_[0]),
                                            key=lambda l:l[1],reverse=True)]

sortedFeatures_axe2 = [i[0] for i in sorted(zip(features,pca.components_[1]),
                                            key=lambda l:l[1],reverse=True)]

sortedFeatures_axe3 = [i[0] for i in sorted(zip(features,pca.components_[2]),
                                            key=lambda l:l[1],reverse=True)]

sortedFeatures_axe4 = [i[0] for i in sorted(zip(features,pca.components_[3]),
                                            key=lambda l:l[1],reverse=True)]
    
print(sortedFeatures_axe1[:1])    
print(sortedFeatures_axe2[:1])  
print(sortedFeatures_axe3[:1])    
print(sortedFeatures_axe4[:1])  

from sklearn.model_selection import train_test_split

train_features, test_features, train_target, test_target = train_test_split(X,
                                                                            y,
                                                                            test_size=0.25,
                                                                            stratify=y)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import auc,roc_curve

model_regression = LogisticRegression()
model_regression.fit(train_features,train_target)
predictions = model_regression.predict(test_features)
probalities = model_regression.predict_proba(test_features)

accuracy = predictions == test_target

true_positives = (predictions==1) & (test_target==1)
false_positives = (predictions==1) & (test_target==0)
true_negatives = (predictions==0) & (test_target==0)
false_negatives = (predictions==0) & (test_target==1)

sensitivity = np.sum(true_positives) / (np.sum(true_positives)+np.sum(false_negatives))

specificity = np.sum(true_negatives) / (np.sum(true_negatives) + np.sum(false_positives))

print('Sensibility of predictions is about : {}'.format(sensitivity))
print('Specificity of predictions is about : {}'.format(specificity))

fpr, tpr, thresholds = roc_curve(test_target, probalities[:,1], pos_label=1)
auc = auc(fpr,tpr)

print('Perimeter of AUC curb : {}'.format(auc))

plt.plot(fpr,tpr,label='AUC={}'.format(auc))
plt.plot([0,1],[0,1],color='red')
plt.legend(loc='lower right')
plt.show()

#######################################################

X_reduced = ['cumul of wafers', 
               'day since emptying', 
               'Is Mono', 
               'day of emptying',
               'WafersProduction']

X = data_maintenance.loc[:,X_reduced]
y = data_maintenance.loc[:,target]

X = StandardScaler().fit_transform(X)

train_features, test_features, train_target, test_target = train_test_split(X,
                                                                            y,
                                                                            test_size=0.25,
                                                                            stratify=y)

from sklearn.metrics import auc,roc_curve

model_regression = LogisticRegression()
model_regression.fit(train_features,train_target)
predictions = model_regression.predict(test_features)
probalities = model_regression.predict_proba(test_features)

accuracy = predictions == test_target

true_positives = (predictions==1) & (test_target==1)
false_positives = (predictions==1) & (test_target==0)
true_negatives = (predictions==0) & (test_target==0)
false_negatives = (predictions==0) & (test_target==1)

sensitivity = np.sum(true_positives) / (np.sum(true_positives)+np.sum(false_negatives))

specificity = np.sum(true_negatives) / (np.sum(true_negatives) + np.sum(false_positives))

print('Sensibility of predictions is about : {}'.format(sensitivity))
print('Specificity of predictions is about : {}'.format(specificity))

fprs, tprs, thresholds = roc_curve(test_target, probalities[:,1], pos_label=1)
aucs = auc(fprs,tprs)

print('Perimeter of AUC curb : {}'.format(aucs))

plt.plot(fprs,tprs,label='AUC={}'.format(aucs))
plt.plot([0,1],[0,1],color='red')
plt.legend(loc='lower right')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import auc,roc_curve

rf = RandomForestClassifier(n_estimators=10,
                            random_state=1,
                            class_weight='balanced')

rf.fit(train_features,train_target)
predictions = rf.predict(test_features)
probalities = rf.predict_proba(test_features)

accuracy = predictions == test_target

true_positives = (predictions==1) & (test_target==1)
false_positives = (predictions==1) & (test_target==0)
true_negatives = (predictions==0) & (test_target==0)
false_negatives = (predictions==0) & (test_target==1)

sensitivity = np.sum(true_positives) / (np.sum(true_positives)+np.sum(false_negatives))

specificity = np.sum(true_negatives) / (np.sum(true_negatives) + np.sum(false_positives))

print('Sensibility of predictions is about : {}'.format(sensitivity))
print('Specificity of predictions is about : {}'.format(specificity))

fprs, tprs, thresholds = roc_curve(test_target, probalities[:,1], pos_label=1)
aucs = auc(fprs,tprs)

print('Perimeter of AUC curb : {}'.format(aucs))

plt.plot(fprs,tprs,label='AUC={}'.format(aucs))
plt.plot([0,1],[0,1],color='red')
plt.legend(loc='lower right')
plt.show()

############################################################

def prediction_maintenance(cumul_wafers,days_since_emptying,is_mono,days_of_emptying,WaferProduction):

    data = [cumul_wafers,
            days_since_emptying,
            is_mono,
            days_of_emptying,
            WaferProduction]
    
    data_res = np.asarray(data).reshape(1,-1)
    
    #data = StandardScaler().fit_transform(data_res)
    data = data_res
    prevision = rf.predict(data)
    prevision_proba = rf.predict_proba(data)
    
    if prevision:
        print('With the data given in input, we can predict that a maintenance action is necesary !!!')
    else :
        print('With the data given in input, no need at the moment for a maintenance action...')
    
    print('Probalities of need of action maintenance {}%'.format(prevision_proba[:,1]*100))
    
    return prevision_proba

#
print('--'*20)
# day 1
cumul_wafers = 0
days_since_emptying = 1
is_mono = 0
days_of_emptying= 0
WaferProduction= 0

prediction = prediction_maintenance(cumul_wafers,days_since_emptying,is_mono,days_of_emptying,WaferProduction)
print('--'*20)

# day 24
cumul_wafers = 879000
days_since_emptying = 24
is_mono = 1
days_of_emptying= 0
WaferProduction= 8790000

prediction = prediction_maintenance(cumul_wafers,days_since_emptying,is_mono,days_of_emptying,WaferProduction)
print('--'*20)



