{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activité : Effectuez un nettoyage et une analyse exploratoire de données texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement des bibliothèques utiles pour cette activité :\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = 'stories'    # dossier contenant les données (fichiers '.story')\n",
    "story_ids = []\n",
    "articles = []\n",
    "highlights = []\n",
    "\n",
    "# On balaye la liste des fichiers contenant les données :\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    # Récupération du texte brut :\n",
    "    with open(path + os.path.sep + filename, 'r') as cur_file:\n",
    "        text_raw = cur_file.read()\n",
    "    # Création paire article / highlights :\n",
    "    start_index_highlight = text_raw.find('@highlight')\n",
    "    article_raw = text_raw[:start_index_highlight]\n",
    "    highlights_raw = text_raw[start_index_highlight:]\n",
    "    # Stockage :\n",
    "    story_ids.append(os.path.splitext(filename)[0])    # nom du fichier sans extension\n",
    "    articles.append(article_raw)\n",
    "    highlights.append(highlights_raw)\n",
    "\n",
    "story_ids = np.array(story_ids, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On utilise la fonction 'CountVectorizer' de scikit-learn pour effectuer en une seule opération\n",
    "# à la fois les traitements sur le texte (passage en minuscules, tokenisation, suppression de la\n",
    "# ponctuation, suppression des stopwords) et le calcul des fréquences des tokens.\n",
    "\n",
    "# On crée une fonction 'cvect' pour simplifier l'appel à 'CountVectorizer' :\n",
    "\n",
    "def cvect(corpus, stop_words, max_df):\n",
    "    cvec = CountVectorizer(\n",
    "        lowercase=True,        # passage en minuscules\n",
    "        tokenizer=None,        # on garde le tokenizer par défaut de scikit-learn\n",
    "        token_pattern=r'\\w+',  # tokenisation avec suppression de la ponctuation\n",
    "        stop_words=stop_words, # suppression des stopwords standards\n",
    "        max_df=max_df)         # suppression des stopwords spécifiques intra-corpus\n",
    "    # Calcul des fréquences :\n",
    "    term_freq = cvec.fit_transform(corpus)\n",
    "    # Récupération du vocabulaire (tokens) sous forme d'array classée dans l'ordre des indices :\n",
    "    vocabulary = np.array(zip(*sorted(cvec.vocabulary_.iteritems(), key=lambda (k,v): (v,k)))[0], dtype=object)\n",
    "    return term_freq, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit les stopwords :\n",
    "\n",
    "# - pour les articles :\n",
    "sw_articles = set(nltk.corpus.stopwords.words('english'))    # stopwords standards pour l'anglais\n",
    "max_df_articles = 0.8    # stopwords spécifiques intra-corpus présents dans plus de 80% des documents\n",
    "\n",
    "# - pour les highlights, l'énoncé ne demande pas la suppression des stopwords :\n",
    "sw_highlights = set(['highlight'])    # on élimine juste le mot-clé 'highlight'\n",
    "max_df_highlights = 1.0    # valeur par défaut, pas de stopwords spécifiques intra-corpus\n",
    "\n",
    "# On applique 'cvect' ('CountVectorizer') aux articles, puis aux highlights :\n",
    "articles_freq, articles_vocabulary = cvect(articles, sw_articles, max_df_articles)\n",
    "highlights_freq, highlights_vocabulary = cvect(highlights, sw_highlights, max_df_highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pour le calcul des tf-idf, on utilise la fonction 'TfidfTransformer' de scikit-learn, qui équivaut\n",
    "# à 'TfidfVectorizer' lorsqu'elle est appliquée à la suite de 'CountVectorizer' :\n",
    "\n",
    "articles_tfidf = TfidfTransformer().fit_transform(articles_freq)\n",
    "highlights_tfidf = TfidfTransformer().fit_transform(highlights_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dictionnaire regroupant le nouveau jeu de données d'entraînement :\n",
    "\n",
    "dataset = {\n",
    "    # articles\n",
    "    'articles_tfidf' : articles_tfidf,               # matrice creuse tf-idf (dim = n_stories x n_tokens_articles)\n",
    "    'articles_vocabulary' : articles_vocabulary,     # liste (dim = n_tokens_articles) des features (tokens)\n",
    "    # highlights\n",
    "    'highlights_tfidf' : highlights_tfidf,           # matrice creuse tf-idf (dim = n_stories x n_tokens_highlights)\n",
    "    'highlights_vocabulary' : highlights_vocabulary, # liste (dim = n_tokens_highlights) des features (tokens)\n",
    "    # références des stories (pour traçabilité)\n",
    "    'story_ids' : story_ids                          # liste (dim = n_stories) des noms de fichiers sans extension\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enregistrement du nouveau jeu de données d'entraînement dans un fichier pour usage ultérieur :\n",
    "\n",
    "with open('dataset.pkl', 'wb') as fichier:\n",
    "    mon_pickler = pickle.Pickler(fichier)\n",
    "    mon_pickler.dump(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
