{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center> Exercice : Classifiez du texte </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix,vstack, hstack, save_npz\n",
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset RCV1 est une matrice creuse (sparse), avec 804414 échantillons et 47236 features. Les 23149 premiers échantillons constituent training set. Les derniers 781265 échantillons constituent le test set.\n",
    "\n",
    "Je vais charger uniquement les données de training car les capacités de ma machine sont limités. J'ai aussi choisi les trois algorithmes à tester qui sont les plus rapide. J'ai essayé de tester les autres algorithmes (comme Random Forest, KNN, réseaux de neuronnes, ...), mais ces derniers consomment trop de resources et j'ai du les arrêter aprés quelques heures de traitement. Je ne les ai pas inclus dans ce notebook.\n",
    "\n",
    "Nous commencons déja par charger les données du training set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcv1 = datasets.fetch_rcv1(subset='train', shuffle=True, random_state=0)\n",
    "rcv2 = datasets.fetch_rcv1(subset='test', shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il y a 23149 échantillon et 47236 features au niveau du training set.\n",
    "Le test set contient 10000 échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23149, 47236)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = rcv1.data\n",
    "y_train = rcv1.target\n",
    "X_test = rcv2.data[:10000]\n",
    "y_test = rcv2.target[:10000]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x47236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 751552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a 84 features qui ne contiennent que des zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.sum(axis=0)==0).sum() #84 colonnes sur 47236 qui sont tous à 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a 103 labels  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23149, 103)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les noms de ces labels sont :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16', 'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182', 'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313', 'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11', 'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142', 'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313', 'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT', 'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157', 'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GENT', 'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL', 'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA', 'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141', 'M142', 'M143', 'MCAT']\n"
     ]
    }
   ],
   "source": [
    "print (rcv1.target_names.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lorsqu'il y a plusieurs labels, comme ce dataset, nous pouvons distinguer deux cas :\n",
    "##### 1. Multiclass classification : \n",
    "Dans ce cas, nous assignons un et un seul label à chaque échantillon : un un seul \"1\" et les reste des \"0\" dans chaque ligne de y.\n",
    "##### 2. Multilabel classification :\n",
    "Dans ce cas, nous pouvons assigner plusieurs labels à chaque échantillon : possibilité d'avoir plusieurs \"1\" dans chaque ligne de y.\n",
    "\n",
    "Nous allons ici groupper les échantillons par nombre de labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 734,\n",
       "         2: 6044,\n",
       "         3: 10868,\n",
       "         4: 2183,\n",
       "         5: 1608,\n",
       "         6: 1027,\n",
       "         7: 361,\n",
       "         8: 187,\n",
       "         9: 83,\n",
       "         10: 36,\n",
       "         11: 13,\n",
       "         12: 3,\n",
       "         14: 2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(((y_train.sum(axis=1).ravel()).A1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y uniquement 734 lignes avec un seul \"1\". Presque la moitier des échantillons (10868) contiennent 3 labels en même temps.\n",
    "\n",
    "Ceci monter que notre problème est un problème : __Multi class Classification__\n",
    "\n",
    "Regardons maintenant les labels rares, ou les nombre déchantillons avec un \"1\" est petit (<=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label E132 (N° 39) : 17 échantillon.\n",
      "Label E141 (N° 41) : 12 échantillon.\n",
      "Label E142 (N° 42) : 8 échantillon.\n",
      "Label E312 (N° 49) : 0 échantillon.\n",
      "Label E313 (N° 50) : 3 échantillon.\n",
      "Label E61 (N° 57) : 15 échantillon.\n",
      "Label G156 (N° 66) : 2 échantillon.\n",
      "Label G159 (N° 69) : 2 échantillon.\n",
      "Label GFAS (N° 77) : 6 échantillon.\n",
      "Label GMIL (N° 80) : 0 échantillon.\n",
      "Label GOBIT (N° 81) : 13 échantillon.\n",
      "Label GTOUR (N° 88) : 23 échantillon.\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "for i in range(0,103):\n",
    "    x.append([i,y_train[:,i].nonzero()[0].shape[0]])\n",
    "for e in x:\n",
    "    if e[1] <=30:\n",
    "        print(\"Label %s (N° %s) : %s échantillon.\" %(rcv1.target_names[e[0]], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nous remarquons qu'il n'y a aucun échantillon avec les labels 49 et 80 (E312 et EGMIL)\n",
    "Normalement, nous devons suuprimer ces colonnes rares de y, et voir aussi s'il y a des outliers et les supprimer si c'est le cas afin que nos algorithmes performent mieux. Mais, dans cet exercice, ce qui est demandé est de tester au moins trois algorithmes avec validation croisée sur ces données. Ainsi, nous allons laisser les données sans traitement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23149x47236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1757801 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x47236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 751552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23149x103 sparse matrix of type '<class 'numpy.uint8'>'\n",
       "\twith 73697 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons déja tester un dummy classifier (strétégie naive) et voir le score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(DummyClassifier(random_state=0))\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La stratégie naive ne donne pas de bons résultats.\n",
    "\n",
    "Testons maintenant le SGDCClassifier (stochastic gradient descent) avec loss=log (logistic regression) qui est une implémentation assez rapide pour les grands datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SGDClassifier(alpha=1e-05, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=0, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(SGDClassifier(loss='log', max_iter=5, alpha=0.00001, random_state=0))\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4962"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la resgression logistique normale, elle prend beaucoup de temps, c'est pour ça que je ne l'ai pas inclue dans ma liste d'algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=300, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(max_iter=300, solver='sag', random_state=0))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4156"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'implémentation SGDClassifier est beaucoup plus rapide et donne des meilleurs performances, j'ai choisi de l'inclure dans ma liste.\n",
    "\n",
    "Nous allons tester maintenannt le SVM linéaire. J'ai choisi l'implémentation LinearSVC qui marche mieux pour les grands datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5201"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le LinearSVC à donné un bon résultat.\n",
    "\n",
    "### J'ai choisi de tester 4 algorithmes :\n",
    "### 1. Le SVM linéaire (LinearSVM)\n",
    "### 2. La regression logistique avec l'implémentation stochastic gradient descent (SGD, loss='log')\n",
    "### 3. Le naive base algorithme (MultinomialNB)\n",
    "### 4. Le SVM linèaire avec l'implémentation stochastic gradient descent (SGD, loss='hinge')\n",
    "\n",
    "### Le choix est dicté par les ressources trés limités de ma machine et de la nature du dataset (Multi label) qui réduit le nombre de choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifiers_list = [\n",
    "   OneVsRestClassifier(SGDClassifier(loss='log', random_state=0)),\n",
    "   OneVsRestClassifier(MultinomialNB()),\n",
    "   OneVsRestClassifier(LinearSVC(random_state=0)),\n",
    "   OneVsRestClassifier(SGDClassifier(loss='hinge', random_state=0))\n",
    "   #OneVsRestClassifier(RandomForestClassifier(random_state=0)),\n",
    "   #MLPClassifier(random_state=0)\n",
    "]\n",
    "\n",
    "grid_params = [ \n",
    "    {\n",
    "     'estimator__max_iter': [5,10,15,20],\n",
    "     'estimator__alpha': [0.00001, 0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    {\n",
    "     'estimator__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    },\n",
    "    {\n",
    "     'estimator__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "    },\n",
    "    {\n",
    "     'estimator__max_iter': [5,10,15,20],\n",
    "     'estimator__alpha': [0.00001, 0.0001, 0.001, 0.01]\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je définit ici la fonction cv_search que je vais utiliser pour avoir le meilleur hyper paramétrage de mes algorithmes en utilisant la cross validation en utilisant GridSearchCV. Le score utilisé est l'accuracy. Nous aurions pu choisir d'autres indicateurs comme AUC ou F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_info(clf, X_test, y_test):\n",
    "    print (\"Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement:\")\n",
    "    print (clf.best_params_)\n",
    "    # Afficher les performances correspondantes\n",
    "    print (\"Résultats de la validation croisée :\")\n",
    "    for mean, std, params in zip(clf.cv_results_['mean_test_score'], clf.cv_results_['std_test_score'], \n",
    "                                clf.cv_results_['params']\n",
    "                                ):\n",
    "        print (\"\\t%s = %0.3f (+/-%0.03f) for %r\" % ('accuracy', mean,std * 2,params))\n",
    "    s = clf.score(X_test,y_test)\n",
    "    print (\"\\nSur le jeu de test : %0.3f\" % s)\n",
    "def cv_search(model, parameters, X_train, y_train, X_test,y_test):\n",
    "    clf = GridSearchCV(model, parameters, cv=5, scoring=\"accuracy\", n_jobs=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    log_info(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Commençons par l'algorithme SGDClassifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement:\n",
      "{'estimator__alpha': 1e-05, 'estimator__max_iter': 5}\n",
      "Résultats de la validation croisée :\n",
      "\taccuracy = 0.549 (+/-0.022) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.547 (+/-0.021) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.545 (+/-0.021) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.545 (+/-0.022) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 20}\n",
      "\taccuracy = 0.373 (+/-0.020) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.371 (+/-0.021) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.369 (+/-0.021) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.370 (+/-0.020) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 20}\n",
      "\taccuracy = 0.054 (+/-0.002) for {'estimator__alpha': 0.001, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.053 (+/-0.002) for {'estimator__alpha': 0.001, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.053 (+/-0.002) for {'estimator__alpha': 0.001, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.053 (+/-0.002) for {'estimator__alpha': 0.001, 'estimator__max_iter': 20}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 20}\n",
      "\n",
      "Sur le jeu de test : 0.496\n"
     ]
    }
   ],
   "source": [
    "cv_search(classifiers_list[0], grid_params[0], X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuite L'algorithme naive base :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement:\n",
      "{'estimator__alpha': 0.0001}\n",
      "Résultats de la validation croisée :\n",
      "\taccuracy = 0.396 (+/-0.015) for {'estimator__alpha': 1e-05}\n",
      "\taccuracy = 0.399 (+/-0.015) for {'estimator__alpha': 0.0001}\n",
      "\taccuracy = 0.396 (+/-0.008) for {'estimator__alpha': 0.001}\n",
      "\taccuracy = 0.386 (+/-0.011) for {'estimator__alpha': 0.01}\n",
      "\taccuracy = 0.368 (+/-0.019) for {'estimator__alpha': 0.1}\n",
      "\taccuracy = 0.099 (+/-0.008) for {'estimator__alpha': 1}\n",
      "\n",
      "Sur le jeu de test : 0.306\n"
     ]
    }
   ],
   "source": [
    "cv_search(classifiers_list[1], grid_params[1], X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuite le SVM linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement:\n",
      "{'estimator__C': 1}\n",
      "Résultats de la validation croisée :\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__C': 0.001}\n",
      "\taccuracy = 0.124 (+/-0.011) for {'estimator__C': 0.01}\n",
      "\taccuracy = 0.480 (+/-0.023) for {'estimator__C': 0.1}\n",
      "\taccuracy = 0.578 (+/-0.023) for {'estimator__C': 1}\n",
      "\taccuracy = 0.560 (+/-0.017) for {'estimator__C': 10}\n",
      "\taccuracy = 0.527 (+/-0.015) for {'estimator__C': 100}\n",
      "\taccuracy = 0.484 (+/-0.013) for {'estimator__C': 1000}\n",
      "\taccuracy = 0.472 (+/-0.016) for {'estimator__C': 10000}\n",
      "\n",
      "Sur le jeu de test : 0.520\n"
     ]
    }
   ],
   "source": [
    "cv_search(classifiers_list[2], grid_params[2], X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Et enfin le SVM linéaire avec l'implémentation SGD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 49 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Continuum\\Anaconda3_64\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 80 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement:\n",
      "{'estimator__alpha': 1e-05, 'estimator__max_iter': 10}\n",
      "Résultats de la validation croisée :\n",
      "\taccuracy = 0.565 (+/-0.018) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.569 (+/-0.024) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.568 (+/-0.018) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.567 (+/-0.019) for {'estimator__alpha': 1e-05, 'estimator__max_iter': 20}\n",
      "\taccuracy = 0.537 (+/-0.020) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.540 (+/-0.023) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.540 (+/-0.022) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.540 (+/-0.022) for {'estimator__alpha': 0.0001, 'estimator__max_iter': 20}\n",
      "\taccuracy = 0.237 (+/-0.019) for {'estimator__alpha': 0.001, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.238 (+/-0.018) for {'estimator__alpha': 0.001, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.238 (+/-0.019) for {'estimator__alpha': 0.001, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.239 (+/-0.019) for {'estimator__alpha': 0.001, 'estimator__max_iter': 20}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 5}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 10}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 15}\n",
      "\taccuracy = 0.000 (+/-0.000) for {'estimator__alpha': 0.01, 'estimator__max_iter': 20}\n",
      "\n",
      "Sur le jeu de test : 0.511\n"
     ]
    }
   ],
   "source": [
    "cv_search(classifiers_list[3], grid_params[3], X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous pouvons conclure que le meilleur algorithme des quatres est le SVM Linéaire avec un score de 52%.\n",
    "### Ceci peut être expliqué par la grande dimension de l'espace de définition (plus que 47000). Le nuage de points sera plus éparpillé, ce qui rend les hyperplans de séparation plus perfomants.\n",
    "### Malheureusement, à cause des capacités trés réduite de ma machine locale, il n'était pas possible de tester d'autres algorithmes intéressants comme les réseaux de neuronnes et les méthodes ensemblistes comme le random forest, ni de tester sur un traning set plus grand vu qu'on dispose de plus de 800000 échantillons.\n",
    "### Cependant, un score entre 50% et 55% est satisfaisant pour ce type de problèmes.\n",
    "\n",
    "## Quelques idées supplémentaires (hors exercice) :\n",
    "\n",
    "Nous n'avons pas effectué de feature engineering supplémentaires. Mais, nous pouvons par exemple tester une réduction de dimentions avec le PCA. Ici par exemple, j'ai utilisé TruncatedSVD qui est plus rapide pour les grands datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7309028627684504"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components=3000, random_state=0)\n",
    "X_res = pca.fit_transform(X_train)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous remarquons que 3000 composants permettent d'expliquer 73% de la variance.\n",
    "\n",
    "Il est aussi interessant supprimer les labels rares, et voir s'il y a des outliers (http://scikit-learn.org/stable/modules/outlier_detection.html) et les supprimer.\n",
    "\n",
    "Pour la classe des problèmes de type __Multi label Classification__, il y a une bibliothéque dédiée développée au dessus de scikit learn qui s'appelle scikit-multilearn (voir le site dédié http://scikit.ml ). Nous testons ici à titre d'illustration le KNN (attention, il faudra l'installer avant. Elle n'est pas incluse dans anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLkNN(ignore_first_neighbours=0, k=1, s=1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "clf = MLkNN(k=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4484"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
