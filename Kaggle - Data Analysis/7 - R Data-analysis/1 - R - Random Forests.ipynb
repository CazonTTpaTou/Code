{
  "cells": [
    {
      "metadata": {
        "_uuid": "3b0accb9d9b71ba65b924582a4ba61670e0a16a4",
        "_cell_guid": "818e707e-ca88-4de8-ac90-81417c5c51f2"
      },
      "cell_type": "markdown",
      "source": "# Welcome to Data Science in R! \n\nThis is part of Kaggle's *[Welcome to Data Science in R!](https://www.kaggle.com/learn/r)* series.\n\nPart 6 of 7: A different type of model: Random forests\n\n___\n<center> \n__[Back to the Table of Contents](https://www.kaggle.com/learn/r)__\n\n------"
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_cell_guid": "ae2dfbf9-8441-433f-901a-73ffef5c4141",
        "_uuid": "61b8dc339d1128131d45caf69e1cd33aa7e175a5",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# load in packages we'll use\nlibrary(tidyverse) # utility functions\nlibrary(rpart) # for regression trees\nlibrary(randomForest) # for random forests\n\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data <- read_csv(\"../input/melb_data.csv\") \n\n# The Melbourne data has some missing values (some houses for which some variables \n# weren't recorded.) We'll learn to handle missing values in a later tutorial.  \n# Your Iowa data doesn't have missing values in the predictors you use. \n# So we will take the simplest option for now, and drop those houses from our data. \n# Don't worry about this much for now, though the code is:\n\n# dropna drops missing values (think of na as \"not available\")\nmelbourne_data <- na.omit(melbourne_data)\n\n# train a decision tree based on our dataset \nfit <- rpart(Price ~ Rooms + Bathroom + Landsize + BuildingArea +\n             YearBuilt + Lattitude + Longtitude, data = melbourne_data)\n\n#print(\"Making predictions for the following 5 houses:\")\n#print(head(melbourne_data))\n\n#print(\"The predictions are\")\n#print(predict(fit, head(melbourne_data)))\n\n#print(\"Actual price\")\n#print(head(melbourne_data$Price))\n\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n✔ ggplot2 2.2.1.9000     ✔ purrr   0.2.4     \n✔ tibble  1.4.2          ✔ dplyr   0.7.4     \n✔ tidyr   0.8.0          ✔ stringr 1.2.0     \n✔ readr   1.2.0          ✔ forcats 0.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nrandomForest 4.6-12\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\nThe following object is masked from ‘package:ggplot2’:\n\n    margin\n\nWarning message:\n“Missing column names filled in: 'X1' [1]”Parsed with column specification:\ncols(\n  .default = col_double(),\n  Suburb = col_character(),\n  Address = col_character(),\n  Type = col_character(),\n  Method = col_character(),\n  SellerG = col_character(),\n  Date = col_character(),\n  CouncilArea = col_character(),\n  Regionname = col_character()\n)\nSee spec(...) for full column specifications.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "6176c9a88a6eaf03f6c594133d3411452c273443",
        "_cell_guid": "0eee778b-39d6-494b-8049-51430389cb27"
      },
      "cell_type": "markdown",
      "source": "# A different type of model: Random forests\n\nDecision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n\nEven today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. One example is the cleverly named Random Forest.\n\nThe random forest uses many trees, and it makes a prediction by average the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters. \n\nOne of the nice thing about R is that the syntax you use to build models across different packages is pretty consistent. All we need to change in order to use a random forest instead of a plain decision tree is to load in the correct library & change the function we use from rpart() to randomForest(), like so:"
    },
    {
      "metadata": {
        "_kg_hide-output": false,
        "_uuid": "da54e7ed7ee0a047d836c4a3657a94dc1a53de3e",
        "trusted": true,
        "_cell_guid": "a188aeb6-e2c0-4671-b6c8-903f7b7697c7"
      },
      "cell_type": "code",
      "source": "# fit a random forest model to our training set\nfitRandomForest <- randomForest(Price ~ Rooms + Bathroom + Landsize + BuildingArea +\n             YearBuilt + Lattitude + Longtitude, data = melbourne_data)\n\n# get the mean average error for our new model, based on our test data\n# mae(model = fitRandomForest, data = melbourne_data)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4456fe265533ddb4e19452141177a008bae0388",
        "_cell_guid": "639dbac2-a9ca-4ff5-b3a4-abb89ac6af25"
      },
      "cell_type": "markdown",
      "source": "### Conclusion \nThere is likely room for further improvement, but this is a big improvement over our previous best decision tree, which was around 320,000 dollars off. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.\n\nYou'll soon learn the XGBoost model, which provides better performance when tuned well with the right parameters (but which requires some skill to get the right model parameters).\n\n___\n\n### Your turn:\nHead over to [your workbook](https://www.kaggle.com/rtatman/welcome-to-data-science-in-r-workbook) and fit a randomForest on your data. You should see a big improvement over your best Decision Tree models.  \n___\n<center> \n__[Back to the Table of Contents](https://www.kaggle.com/learn/r)__\n\n------"
    },
    {
      "metadata": {
        "_uuid": "a671432dd73d19d8c18e7e79533ebf3725eaa1e3",
        "_cell_guid": "7241e737-2647-45e4-a868-f610233ae675"
      },
      "cell_type": "markdown",
      "source": "# Conclusion\n\nNice work! Now that you've got your feet wet with machine learning, you're ready to train new models on new datasets. If you're looking to enter a competition, you can [check out this kernel, that will walk you through putting together your submission file](https://www.kaggle.com/paultimothymooney/welcome-to-data-science-in-r-lesson-8-of-8).\n\nUnfortunately, not all data is as clean as the examples we've been using here. In the next lesson in this series, we'll learn how to use R and the Tidyverse to clean and interact with datasets in a fast, reproducible way."
    },
    {
      "metadata": {
        "_uuid": "68163f9fb13e32e2593a547f5a626737ebbea15d",
        "_cell_guid": "a75ae784-98da-4d65-9dce-514cce842837"
      },
      "cell_type": "markdown",
      "source": "___\n<center> \n[Click here to proceed to the next lesson](https://www.kaggle.com/paultimothymooney/welcome-to-data-science-in-r-lesson-8-of-8)\n___"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.2",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}